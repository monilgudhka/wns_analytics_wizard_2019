{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM wrapper for Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapper Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictionTracker():\n",
    "    \n",
    "    def __init__(self, data_set):\n",
    "        self.__data_set = data_set\n",
    "        self.__predictions = []\n",
    "    \n",
    "    def predict(self, env):\n",
    "        model = env.model\n",
    "        model = model.boosters[model.best_iteration]\n",
    "        pred = model.predict(self.__data_set.data)\n",
    "        self.__predictions.append(pred)\n",
    "        \n",
    "    def get_predictions(self):\n",
    "        return self.__predictions\n",
    "\n",
    "def cv_predict(params, train_set, num_boost_round=100,\n",
    "       folds=None, nfold=5, stratified=True, shuffle=True,\n",
    "       metrics=None, fobj=None, feval=None, init_model=None,\n",
    "       feature_name='auto', categorical_feature='auto',\n",
    "       early_stopping_rounds=None, fpreproc=None,\n",
    "       verbose_eval=None, show_stdv=True, seed=0,\n",
    "       callbacks=None):\n",
    "    \"\"\"Perform the cross-validation with given paramaters.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    params : dict\n",
    "        Parameters for Booster.\n",
    "    train_set : Dataset\n",
    "        Data to be trained on.\n",
    "    num_boost_round : int, optional (default=100)\n",
    "        Number of boosting iterations.\n",
    "    folds : generator or iterator of (train_idx, test_idx) tuples, scikit-learn splitter object or None, optional (default=None)\n",
    "        If generator or iterator, it should yield the train and test indices for the each fold.\n",
    "        If object, it should be one of the scikit-learn splitter classes\n",
    "        (http://scikit-learn.org/stable/modules/classes.html#splitter-classes)\n",
    "        and have ``split`` method.\n",
    "        This argument has highest priority over other data split arguments.\n",
    "    nfold : int, optional (default=5)\n",
    "        Number of folds in CV.\n",
    "    stratified : bool, optional (default=True)\n",
    "        Whether to perform stratified sampling.\n",
    "    shuffle: bool, optional (default=True)\n",
    "        Whether to shuffle before splitting data.\n",
    "    metrics : string, list of strings or None, optional (default=None)\n",
    "        Evaluation metrics to be monitored while CV.\n",
    "        If not None, the metric in ``params`` will be overridden.\n",
    "    fobj : callable or None, optional (default=None)\n",
    "        Custom objective function.\n",
    "    feval : callable or None, optional (default=None)\n",
    "        Customized evaluation function.\n",
    "        Should accept two parameters: preds, train_data.\n",
    "        For multi-class task, the preds is group by class_id first, then group by row_id.\n",
    "        If you want to get i-th row preds in j-th class, the access way is preds[j * num_data + i].\n",
    "        Note: should return (eval_name, eval_result, is_higher_better) or list of such tuples.\n",
    "        To ignore the default metric corresponding to the used objective,\n",
    "        set ``metrics`` to the string ``\"None\"``.\n",
    "    init_model : string, Booster or None, optional (default=None)\n",
    "        Filename of LightGBM model or Booster instance used for continue training.\n",
    "    feature_name : list of strings or 'auto', optional (default=\"auto\")\n",
    "        Feature names.\n",
    "        If 'auto' and data is pandas DataFrame, data columns names are used.\n",
    "    categorical_feature : list of strings or int, or 'auto', optional (default=\"auto\")\n",
    "        Categorical features.\n",
    "        If list of int, interpreted as indices.\n",
    "        If list of strings, interpreted as feature names (need to specify ``feature_name`` as well).\n",
    "        If 'auto' and data is pandas DataFrame, pandas categorical columns are used.\n",
    "        All values in categorical features should be less than int32 max value (2147483647).\n",
    "        Large values could be memory consuming. Consider to use consecutive integers started from zero.\n",
    "        All negative values in categorical features will be treated as missing values.\n",
    "    early_stopping_rounds: int or None, optional (default=None)\n",
    "        Activates early stopping.\n",
    "        CV score needs to improve at least every ``early_stopping_rounds`` round(s)\n",
    "        to continue.\n",
    "        Requires at least one metric. If there's more than one, will check all of them.\n",
    "        Last entry in evaluation history is the one from best iteration.\n",
    "    fpreproc : callable or None, optional (default=None)\n",
    "        Preprocessing function that takes (dtrain, dtest, params)\n",
    "        and returns transformed versions of those.\n",
    "    verbose_eval : bool, int, or None, optional (default=None)\n",
    "        Whether to display the progress.\n",
    "        If None, progress will be displayed when np.ndarray is returned.\n",
    "        If True, progress will be displayed at every boosting stage.\n",
    "        If int, progress will be displayed at every given ``verbose_eval`` boosting stage.\n",
    "    show_stdv : bool, optional (default=True)\n",
    "        Whether to display the standard deviation in progress.\n",
    "        Results are not affected by this parameter, and always contains std.\n",
    "    seed : int, optional (default=0)\n",
    "        Seed used to generate the folds (passed to numpy.random.seed).\n",
    "    callbacks : list of callables or None, optional (default=None)\n",
    "        List of callback functions that are applied at each iteration.\n",
    "        See Callbacks in Python API for more information.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    eval_hist : dict\n",
    "        Evaluation history.\n",
    "        The dictionary has the following format:\n",
    "        {'predict': [values], metric1-mean': [values], 'metric1-stdv': [values],\n",
    "        'metric2-mean': [values], 'metric2-stdv': [values],\n",
    "        ...}.\n",
    "    \"\"\"\n",
    "    if train_set is None:\n",
    "        raise TypeError(\"cv() missing 1 required positional argument: 'train_set'\")\n",
    "    if not isinstance(train_set, lightgbm.Dataset):\n",
    "        raise TypeError(\"Traninig only accepts Dataset object\")\n",
    "    \n",
    "    train_set.free_raw_data = False\n",
    "    tracker = PredictionTracker(data_set=train_set)\n",
    "    \n",
    "    if callbacks is None:\n",
    "        callbacks = []\n",
    "    callbacks.append(tracker.predict)\n",
    "    \n",
    "    results = lightgbm.cv(params, train_set, num_boost_round=num_boost_round,\n",
    "       folds=folds, nfold=nfold, stratified=stratified, shuffle=shuffle,\n",
    "       metrics=metrics, fobj=fobj, feval=feval, init_model=init_model,\n",
    "       feature_name=feature_name, categorical_feature=categorical_feature,\n",
    "       early_stopping_rounds=early_stopping_rounds, fpreproc=fpreproc,\n",
    "       verbose_eval=verbose_eval, show_stdv=show_stdv, seed=seed,\n",
    "       callbacks=callbacks)\n",
    "    \n",
    "    results['predict'] = tracker.get_predictions()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(237609, 15)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_cols = [\"user_id\", \"app_code\", \"os_version\", \"is_4G\", \"freq_cat3\", \"unique_cat3\", \"unique_device\", \n",
    "                \"unique_items\", \"total_sessions\", \"last_active_session\", \"std_price\", \n",
    "                \"app_last_ad_seen\", \"last_ad_seen\", \"overall_click_ratio\", \"app_click_ratio\"]\n",
    "\n",
    "# Reading Training Data\n",
    "training_df = pd.read_csv('data/train/train_feature.csv', index_col='impression_id', parse_dates=['impression_time'])\n",
    "training_df = training_df.fillna(0)\n",
    "\n",
    "X = training_df[feature_cols].values\n",
    "y = training_df['is_click'].values\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'auc-mean': [0.6626283683157459,\n",
       "  0.7006728684115678,\n",
       "  0.7062197206563294,\n",
       "  0.708187091144753,\n",
       "  0.7096309392732094,\n",
       "  0.7108577125886325,\n",
       "  0.7119279533188831,\n",
       "  0.7130464933119954,\n",
       "  0.7134764684188728,\n",
       "  0.7137371093979257,\n",
       "  0.7146664901137294,\n",
       "  0.7150391619167136,\n",
       "  0.7158064450341539,\n",
       "  0.7161736539998955,\n",
       "  0.7163275084570702,\n",
       "  0.7166119954991405,\n",
       "  0.71690547678428,\n",
       "  0.7171375867288378,\n",
       "  0.7173467498303806,\n",
       "  0.7175740340656833,\n",
       "  0.7178255341853521,\n",
       "  0.7177519049811506,\n",
       "  0.7178180920496542,\n",
       "  0.717857306910836,\n",
       "  0.7178881267107904,\n",
       "  0.7182796697279902,\n",
       "  0.7183333752716056,\n",
       "  0.7185608429817945,\n",
       "  0.7186334690610457,\n",
       "  0.7187301271807933,\n",
       "  0.7187761420484128,\n",
       "  0.7190795508074267,\n",
       "  0.7191746650903598,\n",
       "  0.719336323092281,\n",
       "  0.7192973656019891,\n",
       "  0.7195686525846459,\n",
       "  0.7200143256258533,\n",
       "  0.7203629108310567,\n",
       "  0.720414534896831,\n",
       "  0.7203660059515484,\n",
       "  0.7205154657570472,\n",
       "  0.7205946459792616,\n",
       "  0.7208993087196639,\n",
       "  0.7208717700062502,\n",
       "  0.7209713074981672,\n",
       "  0.7209663111686392,\n",
       "  0.7209907451132446,\n",
       "  0.7209838434009633,\n",
       "  0.7209482678473137,\n",
       "  0.7209550700866704,\n",
       "  0.7210462031544072,\n",
       "  0.7211063653678508,\n",
       "  0.7211459393161933,\n",
       "  0.7210674225101239,\n",
       "  0.7210330451943572,\n",
       "  0.7211511493792975,\n",
       "  0.7212109732668606,\n",
       "  0.7213168196978957,\n",
       "  0.7213316784714484,\n",
       "  0.7213045568428819,\n",
       "  0.7212772608737861,\n",
       "  0.7213035466702087,\n",
       "  0.7213738362381794,\n",
       "  0.7212571497055071,\n",
       "  0.7213590199357242,\n",
       "  0.7215138248490014,\n",
       "  0.7216200404723653,\n",
       "  0.7216343221216187,\n",
       "  0.7215507450076636,\n",
       "  0.7215389036493136,\n",
       "  0.721655893192229,\n",
       "  0.7217151305306916,\n",
       "  0.72174266291521,\n",
       "  0.721553390877982,\n",
       "  0.7214205463739731,\n",
       "  0.7214646378332554,\n",
       "  0.7215044584810849,\n",
       "  0.7215378214054282,\n",
       "  0.7215784323310763,\n",
       "  0.7215513443900903,\n",
       "  0.7215144896878177,\n",
       "  0.7215165130584602,\n",
       "  0.7215135737009741,\n",
       "  0.7214990748320942,\n",
       "  0.7214984423622364,\n",
       "  0.7215142315540661,\n",
       "  0.7214848061262729,\n",
       "  0.7215092264210188,\n",
       "  0.7215202000328587,\n",
       "  0.7215013458673794,\n",
       "  0.7214466504820941,\n",
       "  0.7214170239033961,\n",
       "  0.7215023478539405,\n",
       "  0.7214859770786543,\n",
       "  0.7214912624385563,\n",
       "  0.7216718816740949,\n",
       "  0.721765103578961,\n",
       "  0.721669707245173,\n",
       "  0.7216316229392001,\n",
       "  0.7216296419832819],\n",
       " 'auc-stdv': [0.009140747583697957,\n",
       "  0.0043930415126958,\n",
       "  0.005701106427483419,\n",
       "  0.0060633401954096324,\n",
       "  0.006174863420231367,\n",
       "  0.006284166438075269,\n",
       "  0.006081843037278712,\n",
       "  0.005528403201770421,\n",
       "  0.005650548935469577,\n",
       "  0.005817771174253269,\n",
       "  0.006491196837765835,\n",
       "  0.006494128200800578,\n",
       "  0.00631297698646295,\n",
       "  0.0064261371260413685,\n",
       "  0.006142958475129806,\n",
       "  0.0061227585678800866,\n",
       "  0.006311126220010443,\n",
       "  0.0060811179153846635,\n",
       "  0.006174154630432606,\n",
       "  0.0061672775054391555,\n",
       "  0.006216772958255071,\n",
       "  0.006152396609610722,\n",
       "  0.006174803432009439,\n",
       "  0.0060347972146345226,\n",
       "  0.006041464877649348,\n",
       "  0.006006785258085814,\n",
       "  0.006033194071898949,\n",
       "  0.00596900682430201,\n",
       "  0.005939362305852312,\n",
       "  0.0059577190082726605,\n",
       "  0.0057623238489922495,\n",
       "  0.00579186636090615,\n",
       "  0.0057200786594313905,\n",
       "  0.005671210023325407,\n",
       "  0.005531643479546817,\n",
       "  0.005599336179527435,\n",
       "  0.005538392051648704,\n",
       "  0.005363601429873723,\n",
       "  0.005393055280771707,\n",
       "  0.005437944298904169,\n",
       "  0.005350972839884976,\n",
       "  0.005359154564104225,\n",
       "  0.005166726894439437,\n",
       "  0.005122562139111807,\n",
       "  0.005109145424612567,\n",
       "  0.0050442128148817005,\n",
       "  0.005004922436629596,\n",
       "  0.004923802388273687,\n",
       "  0.004882604662918278,\n",
       "  0.004921201696482929,\n",
       "  0.0049486859988195206,\n",
       "  0.00494210377508637,\n",
       "  0.004974947619466965,\n",
       "  0.00487946636984517,\n",
       "  0.004950473484512284,\n",
       "  0.0049621819508196595,\n",
       "  0.005043301122581595,\n",
       "  0.00511209903846233,\n",
       "  0.005116655963068196,\n",
       "  0.005092812366265445,\n",
       "  0.005139039453176574,\n",
       "  0.005157345429967274,\n",
       "  0.00510287621265093,\n",
       "  0.0051744152948537256,\n",
       "  0.005156694196620994,\n",
       "  0.005017862983136597,\n",
       "  0.005007615220687865,\n",
       "  0.005002831444470101,\n",
       "  0.004936438975818403,\n",
       "  0.004905868823535973,\n",
       "  0.004858853191443299,\n",
       "  0.00494956987224992,\n",
       "  0.004947278735930552,\n",
       "  0.004946146507407929,\n",
       "  0.004906306090948774,\n",
       "  0.004938153757828956,\n",
       "  0.004966540637386831,\n",
       "  0.0049851001945314085,\n",
       "  0.004968190433389078,\n",
       "  0.0048679799225997186,\n",
       "  0.004817029891586338,\n",
       "  0.004914718644299626,\n",
       "  0.004848934826346757,\n",
       "  0.004837962833768418,\n",
       "  0.0048040213290547785,\n",
       "  0.004788712384080222,\n",
       "  0.004722865408045149,\n",
       "  0.0046882149433031,\n",
       "  0.004686472780266104,\n",
       "  0.004658765739705341,\n",
       "  0.004589889935481664,\n",
       "  0.004590126721684605,\n",
       "  0.004525233921522388,\n",
       "  0.0044779337303752125,\n",
       "  0.004553556468182887,\n",
       "  0.004591149278268526,\n",
       "  0.004716763734347839,\n",
       "  0.00475964994578847,\n",
       "  0.004707477307539115,\n",
       "  0.00467863213350485],\n",
       " 'predict': [array([0.05266719, 0.04631897, 0.04631897, ..., 0.04631897, 0.04631897,\n",
       "         0.05266719]),\n",
       "  array([0.05994021, 0.04672732, 0.04524374, ..., 0.0483247 , 0.04524374,\n",
       "         0.05994021]),\n",
       "  array([0.06690652, 0.04588725, 0.04440367, ..., 0.04748463, 0.04588176,\n",
       "         0.06690652]),\n",
       "  array([0.06781259, 0.04679332, 0.04530973, ..., 0.04839069, 0.04678782,\n",
       "         0.06781259]),\n",
       "  array([0.06750617, 0.0464869 , 0.04500332, ..., 0.04808428, 0.04648141,\n",
       "         0.06750617]),\n",
       "  array([0.06709253, 0.04607325, 0.04458967, ..., 0.04767063, 0.04606776,\n",
       "         0.06709253]),\n",
       "  array([0.07485886, 0.04485817, 0.04602357, ..., 0.04645555, 0.04485268,\n",
       "         0.07052374]),\n",
       "  array([0.07430771, 0.04430702, 0.04547242, ..., 0.0459044 , 0.04490074,\n",
       "         0.0705718 ]),\n",
       "  array([0.07511149, 0.0451108 , 0.0462762 , ..., 0.04670818, 0.04570452,\n",
       "         0.07137558]),\n",
       "  array([0.07488602, 0.04488533, 0.04605073, ..., 0.04648271, 0.04547905,\n",
       "         0.07115011]),\n",
       "  array([0.0823045 , 0.04598925, 0.04496018, ..., 0.04758663, 0.0443885 ,\n",
       "         0.07448877]),\n",
       "  array([0.09627866, 0.04728507, 0.046256  , ..., 0.04697065, 0.04377253,\n",
       "         0.07992195]),\n",
       "  array([0.0999099 , 0.04698443, 0.04595535, ..., 0.04740616, 0.04347188,\n",
       "         0.07932397]),\n",
       "  array([0.10588345, 0.04778889, 0.04675982, ..., 0.04821062, 0.04236634,\n",
       "         0.08529752]),\n",
       "  array([0.11195926, 0.04746158, 0.0464325 , ..., 0.04788331, 0.04203903,\n",
       "         0.08845925]),\n",
       "  array([0.11175826, 0.04726058, 0.04623151, ..., 0.04768231, 0.04183803,\n",
       "         0.08825825]),\n",
       "  array([0.11928855, 0.04654922, 0.04733671, ..., 0.04878751, 0.04112667,\n",
       "         0.09135856]),\n",
       "  array([0.12558587, 0.04591268, 0.04833665, ..., 0.04815097, 0.04049013,\n",
       "         0.09765589]),\n",
       "  array([0.12963491, 0.04564565, 0.04806963, ..., 0.04788394, 0.0402231 ,\n",
       "         0.10170493]),\n",
       "  array([0.12946398, 0.04547472, 0.04789869, ..., 0.04771301, 0.04005217,\n",
       "         0.10153399]),\n",
       "  array([0.12931005, 0.04532079, 0.04774477, ..., 0.04755908, 0.03989824,\n",
       "         0.10138007]),\n",
       "  array([0.12918959, 0.04520032, 0.0476243 , ..., 0.04743861, 0.03977778,\n",
       "         0.1012596 ]),\n",
       "  array([0.1339898 , 0.04489235, 0.04731633, ..., 0.04713064, 0.03946981,\n",
       "         0.10605981]),\n",
       "  array([0.13832756, 0.05309237, 0.0477951 , ..., 0.04804932, 0.03874466,\n",
       "         0.11039757]),\n",
       "  array([0.13822464, 0.05298945, 0.04769219, ..., 0.04794641, 0.03864175,\n",
       "         0.11029466]),\n",
       "  array([0.13801377, 0.05403981, 0.0467629 , ..., 0.04918607, 0.03840659,\n",
       "         0.11008378]),\n",
       "  array([0.13794492, 0.05397097, 0.04669406, ..., 0.04911723, 0.03833775,\n",
       "         0.11001494]),\n",
       "  array([0.13781904, 0.05384509, 0.04656818, ..., 0.04899135, 0.03821187,\n",
       "         0.10988905]),\n",
       "  array([0.13772205, 0.0537481 , 0.04647119, ..., 0.04889436, 0.03811488,\n",
       "         0.10979207]),\n",
       "  array([0.1392654 , 0.05418337, 0.04737755, ..., 0.04980072, 0.03749415,\n",
       "         0.11133542]),\n",
       "  array([0.13917187, 0.05408984, 0.04728401, ..., 0.04970718, 0.03740061,\n",
       "         0.11124188]),\n",
       "  array([0.14263378, 0.05755175, 0.04728726, ..., 0.04971043, 0.03690027,\n",
       "         0.11074153]),\n",
       "  array([0.14740718, 0.06232516, 0.04809065, ..., 0.05007296, 0.03631042,\n",
       "         0.11195974]),\n",
       "  array([0.14734816, 0.06226614, 0.04803163, ..., 0.05001394, 0.03625139,\n",
       "         0.11190072]),\n",
       "  array([0.1472694 , 0.06218737, 0.04795286, ..., 0.04993518, 0.03617263,\n",
       "         0.11182195]),\n",
       "  array([0.14675397, 0.06325329, 0.04901878, ..., 0.05398386, 0.03599631,\n",
       "         0.11113016]),\n",
       "  array([0.14641885, 0.06291817, 0.04868367, ..., 0.05364874, 0.03560205,\n",
       "         0.11073591]),\n",
       "  array([0.1459089 , 0.06388945, 0.04965495, ..., 0.05573527, 0.03551108,\n",
       "         0.11022923]),\n",
       "  array([0.1458279 , 0.06380846, 0.04957395, ..., 0.05565427, 0.03543009,\n",
       "         0.11014823]),\n",
       "  array([0.14588263, 0.06386318, 0.04962867, ..., 0.05570899, 0.03548481,\n",
       "         0.11020295]),\n",
       "  array([0.14827524, 0.07242301, 0.05121959, ..., 0.0558096 , 0.03500998,\n",
       "         0.10641665]),\n",
       "  array([0.14819185, 0.07233961, 0.05113619, ..., 0.0557262 , 0.03492658,\n",
       "         0.10633325]),\n",
       "  array([0.14809316, 0.07224092, 0.0510375 , ..., 0.05562752, 0.03453071,\n",
       "         0.10593738]),\n",
       "  array([0.14809409, 0.07224185, 0.05103843, ..., 0.05562845, 0.03453164,\n",
       "         0.10593831]),\n",
       "  array([0.14809588, 0.0887973 , 0.05104022, ..., 0.05563023, 0.03453343,\n",
       "         0.1059401 ]),\n",
       "  array([0.14923971, 0.08668288, 0.0489258 , ..., 0.05351581, 0.03458911,\n",
       "         0.10535541]),\n",
       "  array([0.14917243, 0.09762517, 0.04885851, ..., 0.05344853, 0.03452182,\n",
       "         0.10528812]),\n",
       "  array([0.14922236, 0.0976751 , 0.04890844, ..., 0.05349845, 0.03457175,\n",
       "         0.10533805]),\n",
       "  array([0.14920453, 0.09765727, 0.04889061, ..., 0.05348063, 0.03455392,\n",
       "         0.10532022]),\n",
       "  array([0.14940952, 0.11481861, 0.04909561, ..., 0.05323705, 0.03431035,\n",
       "         0.10507665]),\n",
       "  array([0.14771438, 0.11549777, 0.0492886 , ..., 0.0538189 , 0.03409114,\n",
       "         0.12155182]),\n",
       "  array([0.14766767, 0.11545105, 0.04924188, ..., 0.05377219, 0.03404443,\n",
       "         0.1215051 ]),\n",
       "  array([0.14771282, 0.11549621, 0.04928704, ..., 0.05381735, 0.03408959,\n",
       "         0.12155026]),\n",
       "  array([0.14771557, 0.11549896, 0.04928979, ..., 0.0538201 , 0.03409234,\n",
       "         0.12155301]),\n",
       "  array([0.1476533 , 0.11543669, 0.04922752, ..., 0.05375782, 0.03403006,\n",
       "         0.12149074]),\n",
       "  array([0.147592  , 0.11537538, 0.04916621, ..., 0.05369652, 0.03396876,\n",
       "         0.12142944]),\n",
       "  array([0.14763678, 0.11542016, 0.04921099, ..., 0.0537413 , 0.03401354,\n",
       "         0.12147421]),\n",
       "  array([0.14731966, 0.11623135, 0.05023792, ..., 0.05118853, 0.03393063,\n",
       "         0.12250115]),\n",
       "  array([0.14734369, 0.11625538, 0.05026195, ..., 0.05121256, 0.03395465,\n",
       "         0.12252517]),\n",
       "  array([0.14732541, 0.1162371 , 0.05024367, ..., 0.05119428, 0.03393638,\n",
       "         0.1225069 ]),\n",
       "  array([0.14734383, 0.11625553, 0.05026209, ..., 0.0512127 , 0.0339548 ,\n",
       "         0.12252532]),\n",
       "  array([0.1473466 , 0.1162583 , 0.05026486, ..., 0.05121547, 0.03395757,\n",
       "         0.12252809]),\n",
       "  array([0.14733112, 0.11624281, 0.05024938, ..., 0.05119999, 0.03394209,\n",
       "         0.1225126 ]),\n",
       "  array([0.14737064, 0.11628233, 0.0502889 , ..., 0.05123951, 0.03398161,\n",
       "         0.12255212]),\n",
       "  array([0.14740683, 0.11631852, 0.05032509, ..., 0.0512757 , 0.0340178 ,\n",
       "         0.12258831]),\n",
       "  array([0.14720912, 0.11612082, 0.05047615, ..., 0.05107799, 0.03404339,\n",
       "         0.12256409]),\n",
       "  array([0.14687891, 0.1157906 , 0.05014593, ..., 0.05074778, 0.03406944,\n",
       "         0.12254382]),\n",
       "  array([0.14684382, 0.11575551, 0.05011084, ..., 0.05071269, 0.03403435,\n",
       "         0.12250873]),\n",
       "  array([0.14686234, 0.11628949, 0.05012936, ..., 0.05073121, 0.03405287,\n",
       "         0.12252725]),\n",
       "  array([0.14686216, 0.11628931, 0.05012918, ..., 0.05073103, 0.03405269,\n",
       "         0.12252707]),\n",
       "  array([0.14679591, 0.11622306, 0.05006293, ..., 0.05066478, 0.03398644,\n",
       "         0.12246082]),\n",
       "  array([0.14744566, 0.11883007, 0.05071268, ..., 0.05070468, 0.03402634,\n",
       "         0.11688958]),\n",
       "  array([0.14739306, 0.11877747, 0.05066008, ..., 0.05065208, 0.03397373,\n",
       "         0.11683698]),\n",
       "  array([0.14739174, 0.11877615, 0.05065876, ..., 0.05065076, 0.03397242,\n",
       "         0.11683566]),\n",
       "  array([0.1473727 , 0.11875711, 0.05063972, ..., 0.05063172, 0.03395338,\n",
       "         0.11681662]),\n",
       "  array([0.14740931, 0.11879372, 0.05067633, ..., 0.05066833, 0.03398998,\n",
       "         0.11685323]),\n",
       "  array([0.14744488, 0.11882929, 0.0507119 , ..., 0.0507039 , 0.03402556,\n",
       "         0.11688881]),\n",
       "  array([0.14750093, 0.11888534, 0.05076795, ..., 0.05075995, 0.03153935,\n",
       "         0.11694486]),\n",
       "  array([0.14802499, 0.11881499, 0.05129201, ..., 0.0506896 , 0.031469  ,\n",
       "         0.11878595]),\n",
       "  array([0.14853999, 0.11872453, 0.05192394, ..., 0.05059914, 0.03137854,\n",
       "         0.11941787]),\n",
       "  array([0.14855435, 0.11873888, 0.05193829, ..., 0.05061349, 0.03139289,\n",
       "         0.11943222]),\n",
       "  array([0.14607766, 0.11967879, 0.05222156, ..., 0.0481368 , 0.03130801,\n",
       "         0.1197155 ]),\n",
       "  array([0.14610923, 0.11971036, 0.05225313, ..., 0.04816837, 0.03133958,\n",
       "         0.11974707]),\n",
       "  array([0.14610867, 0.1197098 , 0.05225258, ..., 0.04816782, 0.03133902,\n",
       "         0.11974651]),\n",
       "  array([0.14610739, 0.11970852, 0.05225129, ..., 0.04816653, 0.03133774,\n",
       "         0.11974523]),\n",
       "  array([0.14604998, 0.11965111, 0.05219389, ..., 0.04810913, 0.03128033,\n",
       "         0.11968782]),\n",
       "  array([0.14608349, 0.11968462, 0.05222739, ..., 0.04814263, 0.03131384,\n",
       "         0.11972133]),\n",
       "  array([0.14611623, 0.11971736, 0.05226013, ..., 0.04817537, 0.03134658,\n",
       "         0.11975407]),\n",
       "  array([0.14614957, 0.11975071, 0.05229348, ..., 0.04820872, 0.02827496,\n",
       "         0.11978741]),\n",
       "  array([0.1461641 , 0.11976523, 0.052308  , ..., 0.04822324, 0.02828948,\n",
       "         0.11980194]),\n",
       "  array([0.14602027, 0.1196214 , 0.05216417, ..., 0.04807941, 0.02814565,\n",
       "         0.11965811]),\n",
       "  array([0.14597295, 0.11957408, 0.05211685, ..., 0.04803209, 0.02809833,\n",
       "         0.11961079]),\n",
       "  array([0.14584088, 0.11944201, 0.05198479, ..., 0.04790003, 0.02796627,\n",
       "         0.11947872]),\n",
       "  array([0.14581168, 0.11941282, 0.05195559, ..., 0.04787083, 0.02793707,\n",
       "         0.11944952]),\n",
       "  array([0.14584027, 0.1194414 , 0.05198417, ..., 0.04789941, 0.02796566,\n",
       "         0.11947811]),\n",
       "  array([0.14537025, 0.12033491, 0.05315459, ..., 0.04742939, 0.02793458,\n",
       "         0.11924021]),\n",
       "  array([0.14519015, 0.12203861, 0.05415917, ..., 0.04913309, 0.02829503,\n",
       "         0.11960067]),\n",
       "  array([0.14520787, 0.12205633, 0.05417689, ..., 0.04915082, 0.02831276,\n",
       "         0.11961839]),\n",
       "  array([0.14523519, 0.12208364, 0.0542042 , ..., 0.04917813, 0.02834007,\n",
       "         0.1196457 ]),\n",
       "  array([0.14526435, 0.12211281, 0.05423337, ..., 0.0492073 , 0.02836924,\n",
       "         0.11967487])]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    "    'random_state': 1024,\n",
    "    'num_leaves': 22,\n",
    "    'max_depth': 6,\n",
    "    'min_data_in_leaf': 37,\n",
    "    'learning_rate': 0.1,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'categorical_feature': [0,1,2,3,4],\n",
    "    'metric': 'auc'\n",
    "}\n",
    "\n",
    "train_set = lightgbm.Dataset(data=X, label=y)\n",
    "results = cv_predict(params=params, train_set=train_set)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of auc-mean: 100\n",
      "Length of auc-stdv: 100\n",
      "Length of predict: 100\n",
      "Length of 1st predict Element: 237609\n"
     ]
    }
   ],
   "source": [
    "print('Length of auc-mean: {}'.format(len(results['auc-mean'])))\n",
    "print('Length of auc-stdv: {}'.format(len(results['auc-stdv'])))\n",
    "print('Length of predict: {}'.format(len(results['predict'])))\n",
    "print('Length of 1st predict Element: {}'.format(len(results['predict'][0])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
